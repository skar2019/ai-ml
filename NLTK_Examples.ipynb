{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORoHlAYmnP7TsUInUwVMIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skar2019/ai-ml/blob/main/NLTK_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, you’ll need to install NLTK and download the NLTK data package:"
      ],
      "metadata": {
        "id": "6KCiyu7Yhmjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oys0laA_bH4L"
      },
      "outputs": [],
      "source": [
        "# Installation and setup\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')  # You can download specific datasets as needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "Tokenization splits text into words or sentences."
      ],
      "metadata": {
        "id": "ZjPW77HMfT-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello! How are you today? NLP is fascinating.\"\n",
        "word_tokens = word_tokenize(text)  # Word tokenization\n",
        "sentence_tokens = sent_tokenize(text)  # Sentence tokenization\n",
        "\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n"
      ],
      "metadata": {
        "id": "ehm9WbWffUNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KMOFZIyZfdku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and Lemmatization**\n",
        "Simplify words to their root or base form."
      ],
      "metadata": {
        "id": "3Ed6vK38gNH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"running\"\n",
        "print(\"Stemming:\", stemmer.stem(word))\n",
        "print(\"Lemmatization:\", lemmatizer.lemmatize(word, pos=\"v\"))\n"
      ],
      "metadata": {
        "id": "tIU-ow3HgNPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-of-Speech (POS) Tagging**\n",
        "Label each word with its grammatical role."
      ],
      "metadata": {
        "id": "ZoshP3SigRb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is a great toolkit for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "id": "Ck5jihFrgRjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition (NER)**\n",
        "Identify entities such as names, organizations, and locations."
      ],
      "metadata": {
        "id": "F1Lyb37Mgaww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"Apple was founded by Steve Jobs in California.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(tags)\n",
        "\n",
        "print(\"Named Entities:\", named_entities)\n"
      ],
      "metadata": {
        "id": "5FKHbQidga2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wXI8z4highdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**\n",
        "Generate random sentences based on a model or corpus."
      ],
      "metadata": {
        "id": "941CYg1rglZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import genesis\n",
        "from nltk import bigrams, FreqDist, ConditionalFreqDist\n",
        "import random\n",
        "\n",
        "genesis_words = genesis.words('english-kjv.txt')\n",
        "bigrams = nltk.bigrams(genesis_words)\n",
        "cfd = ConditionalFreqDist(bigrams)\n",
        "\n",
        "word = 'God'\n",
        "for i in range(15):\n",
        "    print(word, end=' ')\n",
        "    word = random.choice(list(cfd[word].keys()))\n"
      ],
      "metadata": {
        "id": "FwjW82-Fglhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**\n",
        "Analyze sentiment in text (e.g., positive, negative)."
      ],
      "metadata": {
        "id": "Tzn_tTv3g0s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "text = \"I love NLP! It’s such an interesting field.\"\n",
        "\n",
        "sentiment = sia.polarity_scores(text)\n",
        "print(\"Sentiment Analysis:\", sentiment)\n"
      ],
      "metadata": {
        "id": "S6ve7YrBg01P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}